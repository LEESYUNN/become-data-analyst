{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 一. 分析区域简介\n",
    "\n",
    "我在本次数据清洗项目中选择的区域是中国贵州省贵阳市，是中国西南地区新兴的国家级大数据试验区。之所以选择该地区，一方面是因为它是我的家乡，我比较熟悉；另一方面课程中的案例研究使用的都是外国的城市，所以我选择一个中国的城市进行数据清洗和导入工作，想看看在不同文化环境下有什么样的不同。\n",
    "\n",
    "地图位置及数据导出：\n",
    "\n",
    "* [地图位置](https://www.openstreetmap.org/relation/2782246#map=9/26.7750/106.6965)\n",
    "* [数据导出](http://overpass-api.de/api/map?bbox=105.1996,25.8469,108.1934,27.6981)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 二. 数据整理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bounds': 1,\n",
      " 'member': 24512,\n",
      " 'meta': 1,\n",
      " 'nd': 361665,\n",
      " 'node': 342055,\n",
      " 'note': 1,\n",
      " 'osm': 1,\n",
      " 'relation': 183,\n",
      " 'tag': 66859,\n",
      " 'way': 16618}\n",
      "\n",
      "{'lower': 61283, 'lower_colon': 5271, 'other': 305, 'problemchars': 0}\n"
     ]
    }
   ],
   "source": [
    "import xml.etree.cElementTree as ET\n",
    "import pprint\n",
    "import math\n",
    "import re\n",
    "\n",
    "lower = re.compile(r'^([a-z]|_)*$')\n",
    "lower_colon = re.compile(r'^([a-z]|_)*:([a-z]|_)*$')\n",
    "problemchars = re.compile(r'[=\\+/&<>;\\'\"\\?%#$@\\,\\. \\t\\r\\n]')\n",
    "\n",
    "# count all kinds of tags there\n",
    "def count_tags(filename):\n",
    "    tags = {}\n",
    "    for _, elem in ET.iterparse(filename):\n",
    "        if elem.tag in tags:\n",
    "            tags[elem.tag] += 1\n",
    "        else:\n",
    "            tags[elem.tag] = 1\n",
    "    return tags\n",
    "\n",
    "def key_type(element, keys):\n",
    "    if element.tag == \"tag\":\n",
    "        k = element.attrib['k']\n",
    "        if lower.search(k) is not None:\n",
    "            keys['lower'] += 1\n",
    "        elif lower_colon.search(k) is not None:\n",
    "            keys['lower_colon'] += 1\n",
    "        elif problemchars.search(k) is not None:\n",
    "            keys['problemchars'] += 1\n",
    "        else:\n",
    "            keys['other'] += 1\n",
    "        \n",
    "    return keys\n",
    "\n",
    "def count_keys(filename):\n",
    "    keys = {\"lower\": 0, \"lower_colon\": 0, \"problemchars\": 0, \"other\": 0}\n",
    "    for _, element in ET.iterparse(filename):\n",
    "        keys = key_type(element, keys)\n",
    "\n",
    "    return keys\n",
    "\n",
    "tags = count_tags('guiyang_china.osm')\n",
    "pprint.pprint(tags)\n",
    "print\n",
    "keys = count_keys('guiyang_china.osm')\n",
    "pprint.pprint(keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "整个地图数据有66M，总共840383行。可以从count_tags函数的返回中看到各种不同标签的数量。其中的bounds元素只有一条，它给出了地图数据中所有node元素的经纬度边界：\n",
    "\n",
    "```\n",
    "<bounds minlat=\"25.8469000\" minlon=\"105.1996000\" maxlat=\"27.6981000\" maxlon=\"108.1934000\"/>\n",
    "```\n",
    "\n",
    "后面可以通过这条信息审核一下地图数据中的经纬度坐标是不是落在这个范围内。另外从count_keys函数返回的信息可以看到所有的二级tag元素的key种类都有哪些，其中problemchars类型个数为0，表示地图数据中不存在有问题的key值。使用Udacity项目提供的代码（详见samplek.py）从原始数据中抽取各种大小的元素样本，运行data.py将样本数据转为csv文件并对数据进行人工观察。发现地图数据中至少存在以下问题：\n",
    "\n",
    "1. 一些节点的经纬度不在有效边界范围内（多个数据项之间不匹配，不满足数据质量评估中的“一致性”原则）；\n",
    "2. 一部分道路的英文名称出现了多种英文缩写或拼写错误（数据模式不统一，不满足数据质量评估中的“有效性”原则）；\n",
    "3. 一些节点的name属性有中文，英文和中英文混合三种数据模式（数据模式不统一，不满足数据质量评估中的“有效性”原则）；"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 节点经纬度问题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of error lat 8497, lon 2703\n",
      "latitude diff range [2.00000002337e-07, 4.3298604], longitude diff range [1.20000000017e-05, 4.3298604]\n"
     ]
    }
   ],
   "source": [
    "osm_file = 'guiyang_china.osm'\n",
    "\n",
    "# number of nodes whose latitude is out of bounds\n",
    "count_of_error_lat = 0\n",
    "# number of nodes whose longitude is out of bounds\n",
    "count_of_error_lon = 0\n",
    "# the maximum difference of latitude\n",
    "max_abs_lat_diff = -1\n",
    "# the maximum difference of longitude\n",
    "max_abs_lon_diff = -1\n",
    "# the minimum difference of latitude\n",
    "min_abs_lat_diff = float('inf')\n",
    "# the minimum difference of longitude\n",
    "min_abs_lon_diff = float('inf')\n",
    "\n",
    "for event, element in ET.iterparse(osm_file, events=(\"start\",)):\n",
    "    # retrieve the latitude and longitude range from <bounds .../>\n",
    "    if element.tag == 'bounds':\n",
    "        minlat = float(element.attrib['minlat'])\n",
    "        minlon = float(element.attrib['minlon'])\n",
    "        maxlat = float(element.attrib['maxlat'])\n",
    "        maxlon = float(element.attrib['maxlon'])\n",
    "\n",
    "    # only node elements have latitude and longitude information\n",
    "    if element.tag == 'node':\n",
    "        lat = float(element.attrib['lat'])\n",
    "        lon = float(element.attrib['lon'])\n",
    "        if not (minlat <= lat <= maxlat):\n",
    "            diff = min(math.fabs(lat-minlat), math.fabs(lat-maxlat))\n",
    "            # print('node {} latitude {} out of bounds [{}, {}], diff {}'.format(element.attrib['id'], lat, minlat, maxlat, diff))\n",
    "            count_of_error_lat += 1\n",
    "            if diff > max_abs_lat_diff:\n",
    "                max_abs_lat_diff = diff\n",
    "            if diff < min_abs_lat_diff:\n",
    "                min_abs_lat_diff = diff\n",
    "        if not (minlon <= lon <= maxlon):\n",
    "            diff = min(math.fabs(lat - minlat), math.fabs(lat - maxlat))\n",
    "            # print('node {} longitude {} out of bounds [{}, {}], diff {}'.format(element.attrib['id'], lon, minlon, maxlon, diff))\n",
    "            count_of_error_lon += 1\n",
    "            if diff > max_abs_lon_diff:\n",
    "                max_abs_lon_diff = diff\n",
    "            if diff < min_abs_lon_diff:\n",
    "                min_abs_lon_diff = diff\n",
    "\n",
    "print('total number of error lat {}, lon {}'.format(count_of_error_lat, count_of_error_lon))\n",
    "print('latitude diff range [{}, {}], longitude diff range [{}, {}]'.format(min_abs_lat_diff, max_abs_lat_diff, min_abs_lon_diff, max_abs_lon_diff))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "先来看看经纬度问题，简单写了一个脚本（参见代码check_latlon.py）来对总体数据进行检查，与bounds元素提供的边界值做对比。可以看到总体数据中有8497条数据纬度超出边界范围，另有2703条数据经度超出边界范围，数量上看还是很可观的，看来这部分数据存在问题。但进一步分析发现无论是经度还是纬度其误差范围最大都不超过5度，考虑到GPS等仪器产生的定位误差，我认为这部分数据误差并不严重，对后续的分析工作不造成影响，可以不做处理。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 道路的英文名称问题\n",
    "\n",
    "与案例研究中提供的例子有所不同，贵阳市的地图数据中街道的名字只有一小部分出现在addr:street属性中，绝大部分出现在name，name:en和name:zh中，所以对这三种属性也要进行审核。运行audit.py中的代码，观察发现同一种道路的英文名称出现了两种不同的缩写或者拼写错误：高速公路（Expressway）出现了Expwy和Expy两种缩写，另外还出现了Exprssway这样的拼写错误。另外，对于东南西北方位标识也存在全称和缩写混用的情况。我在data.py代码中做了检查，使用update_name函数（参见audit.py）将所有的道路和方位的命名进行统一规范。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# a mapping for regular expression\n",
    "mapping = { r'\\bSt\\b': r'Street ',\n",
    "            r'\\bSt.\\b': r'Street ',\n",
    "            r'\\bAve\\b': r'Avenue',\n",
    "            r'\\bRd.\\b': r'Road',\n",
    "            r'\\bRd\\b': r'Road',\n",
    "            r'\\bBlvd\\b': r'Boulevard',\n",
    "            r'\\bExpy\\b': r'Expressway',\n",
    "            r'\\bExpwy\\b': r'Expressway',\n",
    "            r'\\bExprssway\\b': r'Expressway',\n",
    "            r'\\bN\\b': r'North',\n",
    "            r'\\bS\\b': r'South',\n",
    "            r'\\bW\\b': r'West',\n",
    "            r'\\bE\\b': r'East',\n",
    "            }\n",
    "\n",
    "def update_name(name, mapping):\n",
    "    for k, v in mapping.items():\n",
    "        name = re.sub(k, v, name)\n",
    "    return name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. name，name:en以及name:zh属性不一致\n",
    "\n",
    "name，name:en和name:zh属性也存在一些问题。首先name属性格式不一致，有的是中文名称，有的是英文名称，有的则包含了中文加英文名称。但是如果将name属性简单清洗只保留中文或英文名称我觉得都不太好，这样会丢失一部分信息，所以应当审核三种名称属性之间是否一致，并在后续的分析中以name:en代表的英文名称或name:zh代表的中文名称为准。这里运行了代码check_name.py来审核name与name:en以及name:zh之间的一致性，具体的方法是通过正则表达式从name中提取中文和英文名称，然后分别检查name:zh和name:en，看数据是否一致，得到的结果如下所示。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "id: 3374791336 name: KFC name_en: 肯德基星力城餐厅 not match\n",
    "id: 4241699404 name: 城关镇 name_zh: 金沙 not match\n",
    "id: 4403132789 name: CCB name_en: China Construction Bank not match\n",
    "id: 4432242971 name: ICBC name_en: Industrial and Commercial Bank of China not match\n",
    "id: 4432242972 name: PSBC name_en: Postal Saving Bank of China not match\n",
    "id: 4432242974 name: RCC name_en: Rural Credit Cooperatives not match\n",
    "id: 4432242975 name: ABC name_en: Agricultural Bank of China not match\n",
    "id: 4432242986 name: CCB name_en: China Construction Bank not match\n",
    "id: 4432243508 name: ABC name_en: Agricultural Bank of China not match\n",
    "id: 4432243509 name: PSBC name_en: Postal Saving Bank of China not match\n",
    "id: 4432243518 name: CCB name_en: China Construction Bank not match\n",
    "id: 4432243527 name: PSBC name_en: Postal Saving Bank of China not match\n",
    "id: 4432243596 name: ICBC name_en: Industrial and Commercial Bank of China not match\n",
    "id: 4432293827 name: ICBC name_en: Industrial and Commercial Bank of China not match\n",
    "id: 4536059675 name: ABC name_en: Agricultural Bank of China not match\n",
    "id: 4536064454 name: CCB name_en: China Construction Bank not match\n",
    "id: 4536064456 name: ICBC name_en: Industrial and Commercial Bank of China not match\n",
    "id: 4537061498 name: CCB name_en: China Construction Bank not match\n",
    "id: 135187642 name: 官井南隧道 name_zh: 官井隧道 not match\n",
    "id: 140408475 name: 六冲河 name_zh: 三岔河 not match\n",
    "id: 140408483 name: 六冲河 name_zh: 三岔河 not match\n",
    "id: 442640875 name: Xihgua Rd name_en: Xihua Rd not match\n",
    "id: 467965990 name: 官井南隧道 name_zh: 官井隧道 not match\n",
    "id: 482578820 name: Daxing Lu name_en: Daxing Road not match\n",
    "id: 482578821 name: Daxing Lu name_en: Daxing Road not match\n",
    "id: 489457817 name: Daxing Lu name_en: Daxing Road not match\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到其中有一部分是银行信息，名称是缩写，比如中国工商银行缩写为ICBC，这些数据在逻辑上是一致的，可以不做处理。有一条关于“城关镇”的数据，name_zh标注的是金沙，说明这里是金沙县的城关镇，这个也无问题。但有些项就有明显的错误了，比如第一项的英文名称标记的是中文名“肯德基星力城餐厅”；“六冲河”和“三岔河”是乌江的两条不同支流，其name属性和name:zh属性不匹配；洗花路英文名称应该为Xihua Rd而不是Xihgua Rd；大兴路英文名应该为Daxing Road而不是Daxing Lu等等，这些都属于录入错误。由于量不是很大且没有明显的规律性，选择手工修正。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 三. 数据概况和探索\n",
    "\n",
    "此外，还计算了不在上述列表中的统计数据。对于 SQL 提交，某些查询使用超过一个表。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 文件大小\n",
    "\n",
    "```\n",
    "guiyang_china.osm...........66M \n",
    "nodes.csv...................27M \n",
    "nodes_tags.csv..............597K\n",
    "openstreetmap.db............33M\n",
    "ways.csv....................955K\n",
    "ways_nodes.csv..............8.5M\n",
    "ways_tags.csv...............1.6M\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 唯一用户的数量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "sqlite> SELECT COUNT(DISTINCT(`uid`)) FROM (SELECT `uid` FROM nodes UNION SELECT `uid` FROM ways) AS u;\n",
    "176\n",
    "```\n",
    "有176个唯一用户"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 节点和途径的数量\n",
    "\n",
    "```\n",
    "sqlite> SELECT COUNT(`id`) FROM nodes;\n",
    "342055\n",
    "sqlite> SELECT COUNT(`id`) FROM ways;\n",
    "16618\n",
    "sqlite> SELECT COUNT(`id`) FROM (SELECT `id` FROM nodes UNION ALL SELECT `id` FROM ways) AS u;\n",
    "358673\n",
    "```\n",
    "\n",
    "节点数量342055，途径数量16618，总共358673条数据。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 节点类型的数量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "sqlite> SELECT `type`, COUNT(DISTINCT(`id`)) AS `num` FROM (SELECT `id`, `type` FROM nodes_tags UNION ALL SELECT `id`, `type` FROM ways_tags) AS `u` GROUP BY `type` ORDER BY `num` DESC;\n",
    "regular|28140\n",
    "name|2467\n",
    "source|328\n",
    "railway|61\n",
    "alt_name|56\n",
    "gns|44\n",
    "generator|39\n",
    "addr|28\n",
    "is_in|8\n",
    "social_facility|7\n",
    "hires|6\n",
    "lanes|6\n",
    "tower|6\n",
    "building|5\n",
    "currency|4\n",
    "alt_ref|1\n",
    "plant|1\n",
    "ref|1\n",
    "roof|1\n",
    "service|1\n",
    "toilets|1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 地图数据上数量最多的生活设施/场所前十名\n",
    "\n",
    "```\n",
    "sqlite> SELECT `value`, COUNT(DISTINCT(`id`)) AS `num` FROM (SELECT `id`, `value` FROM nodes_tags WHERE `key` = 'amenity' UNION SELECT `id`, `value` FROM ways_tags WHERE `key` = 'amenity') AS `u` GROUP BY `value` ORDER BY `num` DESC LIMIT 10;\n",
    "school|90\n",
    "restaurant|63\n",
    "fuel|59\n",
    "bank|48\n",
    "parking|48\n",
    "pharmacy|39\n",
    "hospital|38\n",
    "bus_station|26\n",
    "police|20\n",
    "post_office|19\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 地图数据贡献最大的前二十名用户\n",
    "\n",
    "```\n",
    "sqlite> select user, count(*) as num from (select user from nodes union all select user from ways) as u group by user order by num desc limit 20;\n",
    "katpatuka|128555\n",
    "ff5722|45278\n",
    "Wahsaw|30378\n",
    "Seandebasti|25812\n",
    "jamesks|14019\n",
    "aighes|13401\n",
    "yangfl|13071\n",
    "7thgrade|11637\n",
    "hanchao|7549\n",
    "Tznischd|6220\n",
    "greecemapper|6055\n",
    "ymapper|5470\n",
    "Oberaffe|5331\n",
    "自由分享|4784\n",
    "zhongguo|3988\n",
    "jerryhappy|3729\n",
    "FreedSky|2383\n",
    "Vlad|2288\n",
    "bigalxyz123|2158\n",
    "West Lake|2072\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 四. 数据改进建议\n",
    "\n",
    "## 1. 改进名称相关属性\n",
    "与名称相关的属性主要是tag元素的name，name:en和name:zh属性，这三种数据还是比较脏的。建议分别为name:en和name提供规范的英文名和中文名。这样改进的益处就是name:zh属性可以省略掉，不用维护太多的冗余数据，进而降低数据的出错率。\n",
    "\n",
    "## 2. 提高用户贡献数据的趣味性\n",
    "OpenStreeMap应当给贡献数据的用户某种形式的激励，比如根据贡献率和贡献准确度，引入贡献排行榜之类的机制，鼓励用户多提供准确有效的数据。并建立用户之间的社交关系，形成一个社区或地区性的用户群，这样改进的益处是能够加强用户之间的联系，大家可以互相改进提交的数据，通过协作的方式提高整个地图数据的质量。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 五. 结论\n",
    "\n",
    "数据的质量评估和清洗工作真的是分析工作中非常重要的一个环节，因为分析的正确性很大程度上依赖于数据质量的高低。通过这次地图数据的清洗工作，我发现OpenStreeMap提供的数据在不同国家和地区差别还是蛮大的。比如案例研究中国外城市的街道名称一般放在addr:street中，而我在审核数据时发现自己获取到的地图数据把大部分街道名称放到了name属性中，特别是英文的命名遇到了与案例研究相同的问题：缩写的使用不规范或出现错误。其他类型的数据上传的也很随意。除了相关用户需要改进自己上传数据的质量外，OpenStreeMap也应当使用一些流行的机器学习技术和数据检查技术，来进一步提高数据的质量。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
